---
# Student to edit the student_number, name and surname.
student_number: "225091410"
name: "Ncumisa"
surname: "Fakade"
# Lecturer should set the module, type and number
module: "STAT312"
type: "Data Wrangling and Model Selection"
number: 1
date: "25 July 2025"
# Students should not edit the following...
title: "`r sprintf('%s: %s %02d', rmarkdown::metadata$module, rmarkdown::metadata$type, rmarkdown::metadata$number)`"
author: "`r rmarkdown::metadata$surname`, `r rmarkdown::metadata$name` (`r rmarkdown::metadata$student_number`)"
fontsize: 14pt
papersize: a4paper
output: rmdNMUsimple::nmu_document
header-includes:
  - \usepackage{longtable}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,          # Show code
  warning = FALSE,      # Hide warnings
  message = FALSE,      # Hide messages
  fig.width = 6,        # Figure width
  fig.height = 4,       # Figure height
  fig.align = 'center', # Centre figures
  out.width = '80%'     # Output width
)
```

# Introduction

Welcome to your first practical on data wrangling in R! This session will teach you the basics of using the `dplyr` package to clean and manipulate data. Data wrangling is a key part of data analysis, and you'll spend a lot of time doing it.

Real data is often messy! It has missing values, inconsistencies, and other problems. We'll learn how to fix these step by step using simple functions from `dplyr`.

The functions we'll learn work together using the pipe operator (`%>%`), which makes your code easier to read.

:::{.notebox name="The Pipe Operator"}
The pipe operator (`%>%`) allows you to chain functions together, passing the result of one function as the first argument to the next. This creates readable, left-to-right workflows. For example, `data %>% filter(age > 30) %>% select(name)` is equivalent to `select(filter(data, age > 30), name)`. You can access help with `?magrittr::%>%`.
:::

# Objectives

By the end of this practical, you will be able to:

1. Use basic `dplyr` functions to manipulate data
2. Clean common data issues
3. Create simple workflows to transform data
4. Practice these skills through exercises

:::{.warningbox}
This is a **long** practical! It will take most students **at least 180 minutes**. _Failure to submit a substantially complete practical will negatively affect your participation mark._
:::

# Required Packages

```{r load_packages}
# Load the tidyverse package
library(tidyverse)  # This includes dplyr and other helpful tools
library(lubridate)  # For handling dates
library(knitr)      # For making tables
library(rsample)    # For cross-validation functions
library(broom)      # For model output formatting

```

# Introduction
This combined practical covers the complete workflow from data cleaning to model selection.

# Dataset Overview

We'll work with two datasets:
- `hospital_data_messy.csv`: 210 patient records with common data quality issues
- `retail_sales_data.csv`: 200 store locations for sales prediction

# Part 1: Data Wrangling 

```{r import_data}
# Import the dataset
hospital_data <- read_csv("hospital_data_messy.csv", na = c("NA", ""))

# Look at the structure
glimpse(hospital_data)
```

The dataset has `r nrow(hospital_data)` rows (patients) and `r ncol(hospital_data)` columns (variables).

```{r initialSummary}
# Show first few rows
head(hospital_data)

# Check missing values
hospital_data %>%
  summarise(across(everything(), ~sum(is.na(.)))) %>%
  pivot_longer(
    everything(),
    names_to = "variable",
    values_to = "missing_count"
  ) %>%
  filter(missing_count > 0) %>%
 kable( # kable allows us to make ``nice looking" tables.
   caption = "Missing data summary",
   format = "pandoc"
 )
```

:::{.notebox name="The across Function"}
The `across()` function applies a function to multiple columns simultaneously. Here, `across(everything(), ~sum(is.na(.)))` calculates the number of missing values for every column. The `~` creates an anonymous function, and `.` represents each column being processed. Check `?dplyr::across` for more details.
:::

:::{.notebox name="The everything Helper"}
`everything()` is a selection helper that selects all columns in a dataset. Other useful helpers include `starts_with()`, `ends_with()`, `contains()`, and `where()`. These make column selection more flexible and readable. See `?tidyselect::everything` for the complete list.
:::

:::{.notebox name="Reshaping Data with pivot longer"}
`pivot_longer()` transforms data from wide to long format. Here, it converts our summary of missing values from separate columns into rows with variable names and counts. This format is often more suitable for analysis and visualisation. The reverse operation is `pivot_wider()`. Access help with `?tidyr::pivot_longer`.
:::

# Step 1: Selecting Columns _(Learning objective: select() function)_

The `select()` function lets you choose which columns to keep. This helps focus on the data you need.

First, we'll trim any extra spaces in text columns.

```{r clean_basic}
hospital_clean <- hospital_data %>%
  mutate(across(where(is.character), str_trim))
```

:::{.notebox name="The where Helper Function"}
`where()` selects columns based on a logical condition. Here, `where(is.character)` selects all character (text) columns. You can use any function that returns TRUE or FALSE, such as `where(is.numeric)` or `where(is.factor)`. This provides powerful, condition-based column selection. See `?tidyselect::where` for details.
:::

:::{.notebox name="String Trimming with str trim"}
`str_trim()` removes leading and trailing whitespace from character strings. This is essential for data cleaning as extra spaces can cause matching problems later. The `stringr` package (part of tidyverse) provides many useful string manipulation functions. Check `?stringr::str_trim` for more information.
:::

Example: Select basic patient info.

```{r select_examples}
basic_info <- hospital_clean %>%
  select(patient_id, age, gender)

glimpse(basic_info)
```

**Exercise 1.1**: Create a dataset with patient_id, department, and treatment_cost. Call it `cost_info`.

```{r select_exercise1, eval=FALSE}
# NB: After editing the code below, change the code chunk
# option eval=FALSE to eval=TRUE.
# Replace underscores (_) with the appropriate code
cost_info <- hospital_clean %>%
  select(patient_id, department,treatment_cost)  # Fill in the blanks

# Check
head(cost_info)
```

**Exercise 1.2**: Select all columns from age to department (using : ). Call it `demo_data`.

```{r select_exercise2, eval=FALSE}
# NB: After editing the code below, change the code chunk
# option eval=FALSE to eval=TRUE.
# Replace underscores (_) with the appropriate code
demo_data <- hospital_clean %>%
  select(age:department)

# Check
head(demo_data)
```

:::{.notebox name="Column Range Selection"}
The colon operator (`:`) selects a range of consecutive columns. For example, `age:department` selects all columns from `age` to `department` inclusive, based on their position in the dataset. This is particularly useful when you need several adjacent columns. You can also use negative selection with `-` to exclude columns.
:::

# Step 2: Filtering Rows _(Learning objective: filter() function)_

The `filter()` function picks rows that meet conditions.

Example: Patients over 65.

```{r filter_examples}
elderly_patients <- hospital_clean %>%
  filter(age > 65)

nrow(elderly_patients)
ncol(elderly_patients)
```
```{r}
hospital_clean%>%
  summarise(across(everything(), ~sum(is.na(.))))%>%
kable(
  caption= "missing"
)
```

**Data Check**: Look for problems like impossible ages.

```{r data_quality_check}
suspicious_ages <- hospital_clean %>%
  filter(age < 0 | age > 120)

negative_stay <- hospital_clean %>%
  filter(length_of_stay < 0)

negative_cost <- hospital_clean %>%
  filter(treatment_cost < 0)

zero_cost <- hospital_clean %>%
  filter(treatment_cost == 0)

# Show if any
suspicious_ages
negative_stay
negative_cost
nrow(zero_cost)
```

:::{.notebox name="Logical Operators in filter"}
The `filter()` function uses logical operators: `>` (greater than), `<` (less than), `==` (equals), `!=` (not equals), `|` (OR), and `&` (AND). You can combine multiple conditions to create complex filters. Missing values require special handling with `is.na()`. See `?base::Logic` for complete operator details.
:::

**Exercise 2.1**: Filter patients in "Surgery" department. Call it `surgery_patients`. How many are there?

```{r filter_exercise1, eval=TRUE}
# NB: After editing the code below, change the code chunk
# option eval=FALSE to eval=TRUE.
# Replace underscores (_) with the appropriate code
surgery_patients <- hospital_clean %>%
  filter(department == "Surgery")

cat("Number of surgery patients:", nrow(surgery_patients))
```

**Exercise 2.2**: Filter patients under 30 with length_of_stay > 5. Call it `young_long_stay`.

```{r filter_exercise2, eval=TRUE}
# NB: After editing the code below, change the code chunk
# option eval=FALSE to eval=TRUE.
# Replace underscores (_) with the appropriate code
young_long_stay <- hospital_clean %>%
  filter(age< 30 & length_of_stay> 5)

head(young_long_stay)
```

# Step 3: Transforming Data _(Learning objective: mutate() function)_

The `mutate()` function adds or changes columns.

Example: Clean age and add age group.

```{r mutate_examples}
hospital_transformed <- hospital_clean %>%
  mutate(
    age_clean = ifelse(age < 0 | age > 120, NA, age),
    age_group = case_when(
      age_clean < 18 ~ "Child",
      age_clean >= 18 & age_clean < 65 ~ "Adult",
      age_clean >= 65 ~ "Elderly",
      TRUE ~ "Unknown"
    )
  )

hospital_transformed %>%
  select(patient_id, age, age_clean, age_group) %>%
  head()
```

:::{.notebox name="Conditional Logic with case when"}
`case_when()` provides a powerful way to create multiple conditional statements. It evaluates conditions in order and assigns the first matching result. The `TRUE ~` statement acts as a catch-all for any remaining cases. This is more readable than nested `ifelse()` statements for complex conditions. Check `?dplyr::case_when` for syntax details.
:::

**Handling Dates**: Dates may have different formats. Use `parse_date_time()` from lubridate to standardise them.

First, see some dates.

```{r date_examples}
head(hospital_transformed$admission_date)
```

Now, parse them.

```{r date_parsing}
hospital_with_dates <- hospital_transformed %>%
  mutate(
    admission_date_clean = parse_date_time(
      admission_date, orders = c("dmy", "mdy", "ymd")))

hospital_with_dates %>%
  select(patient_id, admission_date, admission_date_clean) %>%
  head()
```

:::{.notebox name="Date Parsing with parse date time"}
`parse_date_time()` from the lubridate package handles multiple date formats automatically. The `orders` parameter specifies possible formats to try: "dmy" (day-month-year), "mdy" (month-day-year), and "ymd" (year-month-day). The function attempts each format until one succeeds, making it perfect for messy real-world data. See `?lubridate::parse_date_time` for format codes.
:::

**Cleaning Text**: Standardise department names using simple matches.

First, see variations.
```{r}
head(hospital_with_dates)
```

```{r}
hospital_with_dates %>%
  count(department)
```

```{r}

```


```{r string_cleaning}
hospital_with_dates %>%
  count(department)

# Collect together all the different names associated
# with a department
names_for_emergency <- c(
  "emergency", "a&e", "er", "accident & emergency")
names_for_orthopedics <- c(
  "orthopedics", "orthopaedics", "ortho", "bone & joint")
names_for_cardiology <- c("cardiology")
names_for_neurology <- c("neurology")
names_for_surgery <- c("surgery")

# Standardise the department names
hospital_standardised <- hospital_with_dates %>%
  mutate(
    dept_lower = tolower(department),
    department_clean = case_when(
      dept_lower %in% names_for_emergency ~ "Emergency",
      dept_lower %in% names_for_orthopedics ~ "Orthopedics",
      dept_lower %in% names_for_cardiology ~ "Cardiology",
      dept_lower %in% names_for_neurology ~ "Neurology",
      dept_lower %in% names_for_surgery ~ "Surgery",
      TRUE ~ "Unknown"
    ),
    gender = str_to_title(tolower(gender)),
    discharge_status = str_to_title(tolower(discharge_status))
  ) %>%
  select(-dept_lower)

hospital_standardised %>%
  count(department_clean) ##gives a neat standardized count of the department categories
```

:::{.notebox name="The in Operator"}
The `%in%` operator tests whether elements on the left appear anywhere in the vector on the right. For example, `dept_lower %in% names_for_emergency` returns TRUE if the department name matches any value in the emergency names vector. This is much more efficient than multiple `==` conditions connected with `|`. See `?base::%in%` for details.
:::

:::{.notebox name="String Case Conversion"}
Functions like `tolower()`, `toupper()`, and `str_to_title()` standardise text case for consistent matching. `str_to_title()` converts text to title case (First Letter Capitalised). Case standardisation is crucial for data cleaning as "Emergency", "EMERGENCY", and "emergency" should be treated as identical. The stringr package provides additional case conversion options.
:::

**Exercise 3.1**: Add a column `daily_cost = treatment_cost / length_of_stay`. Call the dataset `hospital_with_daily`.

```{r mutate_exercise1, eval=FALSE}
# NB: After editing the code below, change the code chunk
# option eval=FALSE to eval=TRUE.
# Replace underscores (_) with the appropriate code
hospital_with_daily <- hospital_standardised %>%
  mutate(daily_cost = treatment_cost / length_of_stay)
  head(hospital_with_daily) ##with all the columns

hospital_with_daily %>%
  select(patient_id, treatment_cost, length_of_stay, daily_cost) %>% ##with just the selected columns 
  head()
```

**Exercise 3.2**: Add a column `high_cost = ifelse(treatment_cost > 20000, "Yes", "No")`.

```{r mutate_exercise2, eval=FALSE}
# NB: After editing the code below, change the code chunk
# option eval=FALSE to eval=TRUE.
# Replace underscores (_) with the appropriate code
hospital_with_high <- hospital_with_daily %>%
  mutate(high_cost = ifelse(treatment_cost > 20000, "Yes", "No"))

hospital_with_high %>%
  select(patient_id, treatment_cost, high_cost) %>%
  head()
```

# Step 4: Sorting Data _(Learning objective: arrange() function)_

The `arrange()` function sorts rows.

Example: Sort by age descending.

```{r arrange_examples}
by_age_desc <- hospital_standardised %>%
  arrange(desc(age_clean))

head(by_age_desc %>% select(patient_id, age_clean))
```

:::{.notebox name="Sorting with arrange"}
`arrange()` sorts rows based on one or more columns. Use `desc()` for descending order (largest to smallest). Multiple columns create hierarchical sorting: `arrange(department, desc(age))` sorts by department first, then by age within each department. Missing values typically appear last. See `?dplyr::arrange` for advanced sorting options.
:::

**Exercise 4.1**: Sort by treatment_cost descending. Call it `by_cost_desc`.

```{r arrange_exercise1, eval=FALSE}
# NB: After editing the code below, change the code chunk
# option eval=FALSE to eval=TRUE.
# Replace underscores (_) with the appropriate code
by_cost_desc <- hospital_standardised %>%
  arrange(desc(treatment_cost))

by_cost_desc %>%
  select(patient_id, treatment_cost) %>%
  head()##Option A

head(by_cost_desc %>%select(patient_id, treatment_cost)) ##Option B
```

**Exercise 4.2**: Sort by department_clean ascending, then length_of_stay descending.

```{r arrange_exercise2, eval=FALSE}
# NB: After editing the code below, change the code chunk
# option eval=FALSE to eval=TRUE.
# Replace underscores (_) with the appropriate code
by_dept_stay <- hospital_standardised %>%
  arrange(department_clean, desc(length_of_stay))

by_dept_stay %>%
  select(patient_id, department_clean, length_of_stay) %>%
  head()
```

# Step 5: Summarising Data _(Learning objective: summarise() function)_

The `summarise()` function calculates summaries, often with `group_by()`.

Example: Overall average age.

```{r summarise_examples}
overall_avg_age <- hospital_standardised %>%
  summarise(avg_age = mean(age_clean, na.rm = TRUE))

overall_avg_age
```

**Grouped**: Average cost by department.

```{r grouped_summary}
dept_summary <- hospital_standardised %>%
  group_by(department_clean) %>%
  summarise(avg_cost = mean(treatment_cost, na.rm = TRUE))

dept_summary
```

:::{.notebox name="Grouped Operations"}
`group_by()` creates groups within your data, causing subsequent operations to be performed separately for each group. Here, `group_by(department_clean)` means the `summarise()` function calculates separate averages for each department. This is fundamental for comparative analysis. Always check your grouping with `groups()` and remove with `ungroup()` when finished. See `?dplyr::group_by`.
:::

:::{.notebox name="Handling Missing Values in Summaries"}
The `na.rm = TRUE` argument tells summary functions like `mean()`, `sum()`, and `sd()` to ignore missing values. Without this, any missing value causes the entire summary to return `NA`. This parameter is essential when working with real-world data containing missing observations. Check each function's help page for missing value handling options.
:::

**Exercise 5.1**: Calculate total patients and average stay overall. Call it `overall_summary`.

```{r}
hospital_standardised
```


```{r summarise_exercise1, eval=FALSE}
# NB: After editing the code below, change the code chunk
# option eval=FALSE to eval=TRUE.
# Replace underscores (_) with the appropriate code
overall_summary <- hospital_standardised %>%
  summarise(
    total_patients = n(),
    avg_stay = mean(length_of_stay, na.rm = TRUE)
  )

overall_summary
```

:::{.notebox name="The n Function"}
`n()` counts the number of rows in each group. When used without `group_by()`, it counts all rows. When used with grouping, it provides counts for each group separately. This is equivalent to `nrow()` for ungrouped data but works properly with grouped operations. Other useful counting functions include `n_distinct()` for unique values. See `?dplyr::n`.
:::

**Exercise 5.2**: Group by gender, summarise average age and count.

```{r summarise_exercise2, eval=FALSE}
# NB: After editing the code below, change the code chunk
# option eval=FALSE to eval=TRUE.
# Replace underscores (_) with the appropriate code
gender_summary <- hospital_standardised %>%
  group_by(gender) %>%
  summarise(
    avg_age = mean(age, na.rm = TRUE),
    count = n()
  )

gender_summary
```

# Step 6: Putting It Together with Pipes

Use `%>%` to chain steps.

Example: Filter elderly, summarise average cost.

```{r pipe_example}
elderly_avg_cost <- hospital_standardised %>%
  filter(age_group == "Elderly") %>%
  summarise(avg_cost = mean(treatment_cost, na.rm = TRUE))

elderly_avg_cost
```

**Exercise 6**: Filter "Emergency" department, sort by descending treatment_cost, select top 5.

```{r}
hospital_standardised
```


```{r pipe_exercise, eval=FALSE}
# NB: After editing the code below, change the code chunk
# option eval=FALSE to eval=TRUE.
# Replace underscores (_) with the appropriate code
top_emergency <- hospital_standardised %>%
  filter(department== "Emergency") %>%
  arrange(desc(treatment_cost)) %>%
  slice(1:5)  # top 5

top_emergency
```

:::{.notebox name="Row Selection with slice"}
`slice()` selects rows by their position. `slice(1:5)` selects the first five rows, whilst `slice(c(1, 3, 5))` selects specific positions. Related functions include `slice_head()`, `slice_tail()`, `slice_max()`, and `slice_min()` for more targeted selection. This is particularly useful after sorting to get top or bottom values. See `?dplyr::slice` for all variants.
:::

# Step 7: Final Clean Dataset

Here's a full cleaning pipeline.

```{r final_cleaning_pipeline}
# Collect together all the different names associated
# with a department
names_for_emergency <- c("emergency", "a&e", "er",
                         "accident & emergency")
names_for_orthopedics <- c("orthopedics", "orthopaedics",
                           "ortho", "bone & joint")
names_for_cardiology <- c("cardiology")
names_for_neurology <- c("neurology")
names_for_surgery <- c("surgery")

hospital_final <- hospital_data %>%
  mutate(across(where(is.character), str_trim)) %>%
  mutate(
    age_clean = ifelse(age < 0 | age > 120, NA, age),
    length_of_stay = ifelse(length_of_stay < 0, NA, length_of_stay),
    treatment_cost = ifelse(treatment_cost < 0, NA, treatment_cost),
    admission_date_clean = parse_date_time(
      admission_date, orders = c("dmy", "mdy", "ymd")),
    dept_lower = tolower(department),
    department_clean = case_when(
      dept_lower %in% names_for_emergency ~ "Emergency",
      dept_lower %in% names_for_orthopedics ~ "Orthopedics",
      dept_lower %in% names_for_cardiology ~ "Cardiology",
      dept_lower %in% names_for_neurology ~ "Neurology",
      dept_lower %in% names_for_surgery ~ "Surgery",
      TRUE ~ "Unknown"
    ),
    gender = str_to_title(tolower(gender)),
    discharge_status = str_to_title(tolower(discharge_status)),
    age_group = case_when(
      age_clean < 18 ~ "Child",
      age_clean >= 18 & age_clean < 65 ~ "Adult",
      age_clean >= 65 ~ "Elderly",
      TRUE ~ "Unknown"
    ),
    daily_cost = treatment_cost / length_of_stay
  ) %>%
  select(-dept_lower) %>%
  distinct() %>%
  select(patient_id, age_clean, gender, admission_date_clean,
         department_clean, length_of_stay, treatment_cost,
         discharge_status, age_group, daily_cost)

head(hospital_final)
```

:::{.notebox name="Removing Duplicates with distinct"}
`distinct()` removes duplicate rows from your dataset. By default, it considers all columns when identifying duplicates. You can specify particular columns with `distinct(column1, column2)` to remove duplicates based only on those variables. Use `.keep_all = TRUE` to retain all columns when specifying particular variables. This is essential for data quality assurance. See `?dplyr::distinct` for options.
:::

**Export**:

```{r export}
write_csv(hospital_final, "hospital_data_clean.csv")
```

# Transition: From Clean Data to Business Analytics

The cleaning skills you've just mastered aren't just academic—they're essential 
for real business problems. The retail data we'll now analyze has similar issues:
missing values, negative entries, and outliers that need handling before modeling.

Notice how we'll use the same `mutate()`, `ifelse()`, and `filter()` functions
to prepare this data for analysis.

# Part 2: Model Selection 


```{r load-retail}
# Clear workspace from Part 1
rm(list = ls()[!ls() %in% c("hospital_data")])  # Keep only if needed

```

# Business Context

RetailMax is a chain of electronics stores. They want to improve monthly sales performance across their 200 store locations. The analytics team has collected data on various factors that might influence sales. These include advertising spending, staffing levels, store features, and local market conditions.

Your task is to develop a model to predict monthly sales revenue. This will help management make better decisions about resource allocation and planning.

# Dataset Import and Exploration

```{r import_sales_data}
# Import the sales data
sales_data <- read_csv("retail_sales_data.csv")

# Initial exploration
glimpse(sales_data)
```

The dataset contains `r nrow(sales_data)` monthly observations across `r ncol(sales_data)` variables:

- **monthly_sales**: Total sales revenue (R, target variable)
- **advertising_spend**: Monthly advertising spending (R)
- **staff_count**: Number of full-time equivalent employees
- **store_size**: Retail floor space (square metres)
- **foot_traffic**: Monthly customer visits
- **local_income**: Average household income in area (R)
- **competition_distance**: Distance to nearest competitor (km)
- **seasonal_index**: Seasonal adjustment factor (1.0 = average)

```{r explore_data}
# Examine the first few rows
head(sales_data)

# Generate summary statistics
summary(sales_data)
```

**Task 1**: Find potential data quality problems from the summary statistics. Look for:

- Variables with missing values (`NA`s)
- Unrealistic values (negative numbers where they should not exist)
- Extreme outliers that seem impossible

Write your findings below:

**Data Quality Issues Found**

_advertising spend has 11 missing values, foot traffic has 6 missing values, lo
cal income has 5 missing values, there are unrealistic values, extremely negative
minimuns, there extreme outliers in all columns except for seasonal index_

# Data Cleaning

Before building models, fix the data quality issues you found.

## Missing Values

**Hint**: Some variables have missing values that need attention. Think about whether missing values represent real missingness or data entry errors. For this business context, simple imputation methods (such as median imputation for continuous variables) work well.
```{r}
 #Countmissingvaluespervariable
 missing_summary<-sales_data %>%
 summarise_all(~sum(is.na(.))) %>%
 pivot_longer(
   everything(),
   names_to="variables",
   values_to="missing_count"
 )%>%
 filter(missing_count>0)
missing_summary
```

```{r handle_missing, include=TRUE}
# Your code here: Find and handle missing values
# Suggestion: Use is.na() to identify missing patterns
# Consider median or mean imputation for continuous variables

#  

#Count missing values per variable
 missing_summary<-sales_data %>%
 summarise_all(~sum(is.na(.))) %>%
 gather(variables,missing_count)%>%
 filter(missing_count>0)
#missing_summary

 #Identify negative values in variables that should be non-negative
 negative_vars<-c("monthly_sales","advertising_spend","staff_count","store_size",
                  "foot_traffic","local_income","competition_distance")
 
 negative_summary<-sales_data %>%
 select(all_of(negative_vars))%>%
 summarise_all(~sum(.<0, na.rm=TRUE )) %>%
 gather(variables,negative_count)%>%
 filter(negative_count>0)

 #Calculate extreme outliers using IQR method
 outlier_summary<-sales_data %>%
 select_if(is.numeric)%>%
 summarise_all(~ {
    q1<-quantile(.,0.25, na.rm= TRUE)
    q3 <- quantile(., 0.75, na.rm = TRUE)
    iqr <- q3-q1
    sum(.< (q1-1.5*iqr) | . > (q3+1.5*iqr), na.rm = TRUE)
 }) %>%
 gather(variables,outlier_count)%>%
 filter(outlier_count>0)
```

## Outlier Detection

**Hint**: Look at variables for extreme values that could be data entry errors or genuinely unusual observations. Pay special attention to values that are negative when they should be positive, or values that are much larger than typical observations.

```{r handle_outliers}
# Your code here: Find and handle outliers
# Suggestion: Use summary() and boxplots to identify extreme values
# Consider the business context when deciding if values are realistic
# Once identified the outliers, use the IQR method to remove them:
#   IQR Method: Values beyond Q1 - 1.5×IQR or Q3 + 1.5×IQR 
#               may be statistical outliers


# Store original row count for tracking
 original_rows <- nrow(sales_data)
 
 #Handle missing values with median imputation
 sales_data<-sales_data %>%
 mutate(
  advertising_spend= if_else(
    is.na(advertising_spend),
    median(advertising_spend, na.rm = TRUE),
    advertising_spend),
  
  foot_traffic= if_else(
    is.na(foot_traffic),
    median(foot_traffic,na.rm = TRUE),
    foot_traffic),
  
  local_income= if_else(
    is.na(local_income),
    median(local_income,na.rm = TRUE),
    local_income)
 )
 #Verify nomissingvaluesremain
 missing_after_imputation<-sum(is.na(sales_data))
  missing_after_imputation
```
```{r handle_outliers2}


 #Convert negative values to positive (assuming data entry errors)
 sales_data<-sales_data %>%
 mutate(
  monthly_sales= abs(monthly_sales),
  advertising_spend= abs(advertising_spend),
  competition_distance= abs(competition_distance)
 )


 #Function to remove outliers using IQR method
 remove_outliers<-function(data,var) {
 #Calculate quartiles and IQR
 q1<-quantile(data[[var]],0.25, na.rm= TRUE)
 q3<-quantile(data[[var]],0.75, na.rm= TRUE)
 iqr<-q3-q1
 
 #Define outlier bounds (remeber the sum structure)
 lower_bound<-q1-1.5*iqr
 upper_bound<-q3+1.5*iqr
 
 #Filter data to exclude outliers
 filtered_data<-data %>%
 filter(.data[[var]]>= lower_bound& .data[[var]]<= upper_bound)
 return(filtered_data)
 }
 
 #Apply outlier removal to key variables
 sales_data<-sales_data %>%
 remove_outliers("monthly_sales")%>%
 remove_outliers("advertising_spend")%>%
 remove_outliers("staff_count")%>%
 remove_outliers("store_size")%>%
 remove_outliers("foot_traffic")%>%
 remove_outliers("local_income")%>%
 remove_outliers("competition_distance")
 

 # Calculate cleaning impact###########################
 cleaned_rows<- nrow(sales_data)
 rows_removed <- original_rows- cleaned_rows
 
  cleaned_rows
  rows_removed
```

**Task 2**: Explain your data cleaning decisions. Why did you choose specific imputation methods and outlier handling strategies?

**Data Cleaning Explanation:**

_I imputed the missing values using the median because the data was skewed and contained outliers. The mean is sensitive to outliers, which could distort the results. For negative values, I applied the absolute value function to convert them to positive values, as the negatives were likely due to human error and it is more appropriate to work with positive figures in this context. I then removed outliers on a per-column basis, as this approach is more precise and ensures targeted data cleaning._

# Model Specification and Cross-Validation

Build multiple linear regression models with different predictor combinations. Use k-fold cross-validation to compare model performance fairly.

## Model Specifications

Consider these potential model specifications:

1. **Simple Model**: Key predictors only
2. **Full Model**: All available predictors
3. **Optimised Model**: Carefully chosen subset based on business logic

```{r define_models}
# Your code here: Define your model formulas
# Example structure (modify as needed):
 model_1 <- monthly_sales ~ advertising_spend + staff_count
 model_2 <- monthly_sales ~ .  # Full model
 model_3 <- monthly_sales ~ advertising_spend + staff_count + store_size + foot_traffic + seasonal_index

```

## Cross-Validation Implementation

Use 10-fold cross-validation to evaluate each model specification.

**Hint**: Use `rsample::vfold_cv()` to create the cross-validation folds. The `rsample` package works well with `dplyr` workflows using `map()` functions for applying models across folds.

```{r cv_setup}
# Your code here: Set up cross-validation folds
# Suggestion: Use vfold_cv() with v = 10

 set.seed(123) #Forreproducibility
 cv_folds<- vfold_cv(sales_data, v=10)
```

```{r cv_models}
# Your code here: Apply cross-validation to each model
# Suggestion: Create a function that fits a model to training data 
# and calculates performance metrics on validation data
# Use map() to apply this function across all folds

 calc_metrics<-function(split,model_formula) {
  train <- analysis(split)
  test <- assessment(split)
  fit <- lm(model_formula,data = train)
  preds <- predict(fit,newdata =test)
  
 #Calculate performance metrics#############################
 rmse<-sqrt(mean((test$monthly_sales-preds)^2))
 
 mae<-mean(abs(test$monthly_sales-preds))
 
 r2<-1-sum((test$monthly_sales-preds)^2)/sum((test$monthly_sales-mean(test$monthly_sales))^2)
 
 tibble(rmse= rmse,mae = mae, r2 =r2)
 }

 #Apply cross-validation to each model
 results_1 <- cv_folds %>%
  mutate(metrics= map(splits,~calc_metrics(.,model_1))) %>%
  unnest(metrics)
 
 results_2 <- cv_folds %>%
  mutate(metrics= map(splits,~calc_metrics(.,model_2))) %>%
  unnest(metrics)
 
 results_3 <- cv_folds %>%
  mutate(metrics= map(splits,~calc_metrics(.,model_3))) %>%
  unnest(metrics)
```

## Performance Metrics

Calculate and compare relevant performance metrics for each model:

- **RMSE** (Root Mean Square Error): Measures average prediction error
- **MAE** (Mean Absolute Error): Measures average absolute prediction error
- **R²** (R-squared): Proportion of variance explained

```{r calculate_metrics}
# Your code here: Calculate performance metrics for each model
# Compare mean performance across all cross-validation folds

#Summarise metrics with confidence intervals
 summary_1<-results_1 %>%
 summarise(
 rmse_mean= mean(rmse),
 rmse_se =sd(rmse) /sqrt(n()),
 mae_mean =mean(mae),
 mae_se =sd(mae) /sqrt(n()),
 r2_mean =mean(r2),
 r2_se =sd(r2) /sqrt(n())
 ) %>%
 mutate(model= "Simple")

 summary_2<-results_2 %>%
 summarise(
 rmse_mean= mean(rmse),
 rmse_se =sd(rmse) /sqrt(n()),
 mae_mean =mean(mae),
 mae_se =sd(mae) /sqrt(n()),
 r2_mean =mean(r2),
 r2_se =sd(r2) /sqrt(n())
 ) %>%
 mutate(model= "Full")
 
summary_3<-results_3 %>%
 summarise(
 rmse_mean= mean(rmse),
 rmse_se =sd(rmse) /sqrt(n()),
 mae_mean =mean(mae),
 mae_se =sd(mae) /sqrt(n()),
 r2_mean =mean(r2),
 r2_se =sd(r2) /sqrt(n())
 ) %>%
 mutate(model= "Optimised")

#Combine summaries
 all_summaries<-bind_rows(summary_1, summary_2, summary_3)
 
 #Identify best performing model
 best_model_name<-all_summaries%>%
 arrange(rmse_mean)%>%
 slice(1)%>%
 pull(model)

```

**Task 3**: Present your cross-validation results in a clear, professional format. Create a summary table showing mean performance metrics and their standard errors across folds.

```{r results_table}
# Your code here: Create a summary table of results

 #Creat comprehensive summary table
all_summaries %>%
  select(model,rmse_mean, rmse_se, mae_mean,mae_se, r2_mean, r2_se) %>%
  kable(
    digits = c(0, rep(2,6)),# 0 decimal places for the first column ("Model") and Next 6 elements ,,,2 decimal places for the next 6 columns (the performance metrics)
    caption = "Cross-Validation Performance Metrics",
    col.names = c("Model", "RMSE (Mean)", "RMSE (SE)", 
                  "MAE (Mean)", "MAE (SE)", "R2 (Mean)", "R2 (SE)"),
    format = "pandoc"
  )
```

# Model Selection and Interpretation

Based on your cross-validation results, select the best model specification.

**Task 4**: Explain your model selection decision. Consider both statistical performance and business interpretability.

**Model Selection Explanation:**

_Based on the cross-validation results, the full model was selected because it achieved a low RMSE and MAE, a high R², and demonstrated stable performance across all folds. This balance between accuracy and consistency makes it the most suitable model for business use_

## Final Model Analysis

Fit your selected model to the complete dataset and examine the results.

```{r final_model}
# Your code here: Fit the selected model to the full dataset
# Examine coefficient estimates, significance, and overall fit

#Extract performance metrics for best model
 best_metrics<-all_summaries%>%
 filter(model== best_model_name )

 #Calculate performance improvements
 simple_rmse<-all_summaries%>%
  filter(model=="Simple")%>%
  pull(rmse_mean)
 
 best_rmse<-best_metrics%>%
  pull(rmse_mean)
 
 rmse_improvement<-(
         ( simple_rmse-best_rmse)/ simple_rmse) *100
 
 #Compare_all_models
 rmse_comparison<-all_summaries%>%
 select(model,rmse_mean, r2_mean) %>%
 arrange(rmse_mean)

```

```{r}
# Fit the selected model based on CV results
if(best_model_name == "Simple") {
  final_fit <- lm(model_1, data =sales_data)
  selected_formula<-model_1
} else if(best_model_name == "Full") {
  final_fit <- lm(model_2, data = sales_data)
  selected_formula<-model_2
} else {
  final_fit <- lm(model_3, data = sales_data)
  selected_formula<-model_3
}

# Display model summary
summary(final_fit)
```
```{r}
 # Extract key model statistics
 model_stats <- glimpse(final_fit)
 coef_summary <- glimpse(final_fit)
```


**Task 5**: Interpret your final model results in business terms. What do the coefficients tell us about the drivers of sales performance?

**Business Interpretation:**

_The business should lean more into Investments in marketing, staffing, and
strategies to increase store visits can positively impact sales and it appears that Local economic conditions also matter in the consideration of locations since stores in higher-income areas tend to perform better._

# Validation and Diagnostics

Check whether your selected model meets the assumptions of linear regression.

```{r model_diagnostics}
# Your code here: Create diagnostic plots
# Check residual patterns, normality, and influential observations

 # Create diagnostic plots
 par(mfrow = c(2, 2))
 plot(final_fit)
 par(mfrow = c(1, 1))

# Additional diagnostic tests
residuals_data <- tibble(
  fitted = fitted(final_fit),
  residuals = residuals(final_fit),
  standardised_residuals = rstandard(final_fit)
)

# Calculate diagnostic statistics
leverage_threshold <- 2 * length(coef(final_fit)) / nrow(sales_data)
high_leverage <- sum(hatvalues(final_fit) > leverage_threshold)
influential_points <- sum(abs(rstudent(final_fit)) > 2)
```

**Task 6**: Comment on model assumptions and any potential concerns with your final specification.

**Model Diagnostics Assessment:**

_The final model explains about 43% of monthly sales variation (Adjusted R²
= 0.421). Residuals are roughly normal with mostly linear relationships, though
variability slightly increases at higher predicted sales. No extreme outliers weremdetected, but predictions at very high or low sales should be interpreted cautiously_

# Business Recommendations

Based on your analysis, provide actionable recommendations for RetailMax management.

**Task 7**: Create specific, data-driven recommendations for improving sales performance. Consider both the statistical evidence and practical business constraints.

**Management Recommendations:**

1. **Priority Actions:**
   _Increase advertising spend strategically, Optimize staffing
levels, Leverage foot traffic and local income data_

2. **Resource Allocation:**
   _Prioritize investment in advertising channels that
historically drive measurable sales increases and monitor underperforming
stores for potential adjustments in staffing or local marketing strategies._

3. **Performance Monitoring:**
   _Track monthly sales, advertising ROI, staff
efficiency metrics, and foot traffic trends._

4. **Model Limitations:**
   _The model explains 43% of sales variance, leaving a
substantial portion unexplained. Factors like seasonality, competitor activ
ity, or online sales may also influence sales. Future analyses could incorpo
rate additional data, such as promotions, competitor pricing, or customer
demographics, to improve predictive accuracy._

# Conclusion

Cross-validation provides essential protection against overfitting when comparing different model specifications. Your analysis shows how systematic model comparison enables evidence-based decision-making in business contexts.

The step-by-step process of model development, validation, and interpretation ensures that analytical insights translate effectively into actionable business strategy whilst maintaining statistical rigour.

**Task 8**: Reflect on the cross-validation process. How did the CV results influence your model selection compared to simply examining in-sample fit statistics?

**Cross-Validation Reflection:**

_Cross-validation provided a realistic estimate of model performance on new data.
It showed that simpler models performed nearly as well as the full model, helping
avoid overfitting. This confirmed that the chosen model balances predictive
accuracy with interpretability and is likely to generalize well._






